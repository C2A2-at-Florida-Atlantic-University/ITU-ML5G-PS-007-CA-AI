{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2efc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import numpy as np \n",
    "import random \n",
    "import matplotlib \n",
    "matplotlib.use('Agg')  \n",
    "import matplotlib.pyplot as plt \n",
    "import time \n",
    "import sys \n",
    "from math import ceil \n",
    "from sklearn.decomposition import PCA \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import torch \n",
    "from torch import nn \n",
    "from sklearn.metrics import accuracy_score \n",
    "from tqdm.notebook import tqdm as tqdm \n",
    "from brevitas.export.onnx.generic.manager import BrevitasONNXManager \n",
    "from finn.util.inference_cost import inference_cost \n",
    "import json \n",
    "import netron \n",
    "import tensorly as tl\n",
    "import os \n",
    "from IPython.display import IFrame\n",
    " \n",
    "dataset_path = \"./GOLD_XYZ_OSC.0001_1024.hdf5\" \n",
    " \n",
    "gpu = 0 \n",
    "if torch.cuda.is_available(): \n",
    "    torch.cuda.device(gpu) \n",
    "    print(\"Using GPU %d\" % gpu) \n",
    "else: \n",
    "    gpu = None \n",
    "    print(\"Using CPU only\") \n",
    " \n",
    "class radioml_18_dataset(Dataset): \n",
    "    def __init__(self, dataset_path): \n",
    "        super(radioml_18_dataset, self).__init__() \n",
    "        h5_file = h5py.File(dataset_path,'r') \n",
    "        self.data = h5_file['X'] \n",
    "        self.mod = np.argmax(h5_file['Y'], axis=1) # comes in one-hot encoding \n",
    "        self.snr = h5_file['Z'][:,0] \n",
    "        self.len = self.data.shape[0] \n",
    " \n",
    "        self.mod_classes= ['OOK','4ASK','8ASK','BPSK','QPSK','8PSK','16PSK','32PSK',\n",
    "                '16APSK','32APSK','64APSK','128APSK','16QAM','32QAM','64QAM','128QAM','256QAM',\n",
    "                        'AM-SSB-WC','AM-SSB-SC','AM-DSB-WC','AM-DSB-SC','FM','GMSK','OQPSK']\n",
    " \n",
    "        self.snr_classes = np.arange(-20., 32., 2) # -20dB to 30dB \n",
    " \n",
    "        # do not touch this seed to ensure the prescribed train/test split! \n",
    "        np.random.seed(2018) \n",
    "        train_indices = [] \n",
    "        test_indices = [] \n",
    "        for mod in range(0, 24): # all modulations (0 to 23) \n",
    "            for snr_idx in range(0, 26): # all SNRs (0 to 25 = -20dB to +30dB) \n",
    "                # 'X' holds frames strictly ordered by modulation and SNR \n",
    "                start_idx = 26*4096*mod + 4096*snr_idx \n",
    "                indices_subclass = list(range(start_idx, start_idx+4096)) \n",
    "                 \n",
    "                # 90%/10% training/test split, applied evenly for each mod-SNR p\n",
    "                split = int(np.ceil(0.1 * 4096))  \n",
    "                np.random.shuffle(indices_subclass)\n",
    "\n",
    "                train_indices_subclass = indices_subclass[split:] \n",
    "                test_indices_subclass = indices_subclass[:split]\n",
    "                 \n",
    "                # you could train on a subset of the data, e.g. based on the SNR\n",
    "                # here we use all available training samples \n",
    "                if snr_idx >= 0: \n",
    "                    train_indices.extend(train_indices_subclass) \n",
    "                test_indices.extend(test_indices_subclass) \n",
    "\n",
    "        torch.manual_seed(2016) #this seed has provided best performance\n",
    "        self.test_indices = test_indices         \n",
    "        self.train_indices = train_indices \n",
    "        self.train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "        self.test_sampler = torch.utils.data.SubsetRandomSampler(test_indices) \n",
    " \n",
    "    def __getitem__(self, idx): \n",
    "        # transpose frame into Pytorch channels-first format (NCL = -1,2,1024) \n",
    "        return self.data[idx].transpose(), self.mod[idx], self.snr[idx] \n",
    " \n",
    "    def __len__(self): \n",
    "        return self.len \n",
    " \n",
    "dataset = radioml_18_dataset(dataset_path) \n",
    " \n",
    "def init_weights(m): \n",
    "    if isinstance(m, nn.Linear): \n",
    "        torch.nn.init.xavier_uniform_(m.weight) \n",
    "        m.bias.data.fill_(0.01) \n",
    "\n",
    "#Create original baseline network\n",
    "dr = 0.5 \n",
    "model = nn.Sequential( \n",
    "    nn.Conv1d(2, 64, kernel_size=8), \n",
    "    nn.ReLU(), \n",
    "    nn.MaxPool1d(2, stride=2), \n",
    "    nn.Conv1d(64, 64, kernel_size=8), \n",
    "    nn.ReLU(), \n",
    "    nn.MaxPool1d(2, stride=2), \n",
    "    nn.Conv1d(64, 64, kernel_size=8), \n",
    "    nn.ReLU(), \n",
    "    nn.MaxPool1d(2, stride=2), \n",
    "    nn.Conv1d(64, 64, kernel_size=8), \n",
    "    nn.ReLU(), \n",
    "    nn.MaxPool1d(2, stride=2), \n",
    "    nn.Conv1d(64, 64, kernel_size=8), \n",
    "    nn.ReLU(), \n",
    "    nn.MaxPool1d(2, stride=2), \n",
    "    nn.Conv1d(64, 64, kernel_size=8), \n",
    "    nn.ReLU(), \n",
    "    nn.MaxPool1d(2, stride=2), \n",
    "    nn.Conv1d(64, 64, kernel_size=8), \n",
    "    nn.ReLU(), \n",
    "    nn.MaxPool1d(2,2), \n",
    "    nn.Flatten(), \n",
    "    nn.Linear(64, 128), \n",
    "    nn.SELU(), \n",
    "    nn.Dropout(dr), \n",
    "    nn.Linear(128, 128), \n",
    "    nn.SELU(), \n",
    "    nn.Dropout(dr), \n",
    "    nn.Linear(128, 24), \n",
    ")\n",
    "model.apply(init_weights) \n",
    "print(model) \n",
    " \n",
    "def train(model, train_loader, optimizer, criterion): \n",
    "    losses = [] \n",
    "    # ensure model is in training mode \n",
    "    model.train()     \n",
    " \n",
    "    for (inputs, target, snr) in tqdm(train_loader, desc=\"Batches\", leave=False):\n",
    "        if gpu is not None: \n",
    "            inputs = inputs.cuda() \n",
    "            target = target.cuda() \n",
    "        #print(inputs.shape)         \n",
    "        # forward pass \n",
    "        output = model(inputs) \n",
    "        loss = criterion(output, target) \n",
    "        #print(output.shape) \n",
    "        # backward pass + run optimizer to update weights \n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "         \n",
    "        # keep track of loss value \n",
    "        losses.append(loss.cpu().detach().numpy()) \n",
    "            \n",
    "    return losses \n",
    " \n",
    "def test(model, test_loader):     \n",
    "    # ensure model is in eval mode \n",
    "    model.eval()  \n",
    "    y_true = [] \n",
    "    y_pred = [] \n",
    "    with torch.no_grad(): \n",
    "        for (inputs, target, snr) in test_loader: \n",
    "            if gpu is not None: \n",
    "                inputs = inputs.cuda() \n",
    "                target = target.cuda() \n",
    "            output = model(inputs) \n",
    "            #print(output.shape)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            #print(pred.shape)\n",
    "            y_true.extend(target.tolist())  \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "            #print(len(y_true), len(y_pred))\n",
    "    print(y_true[:100], y_pred[:100])\n",
    "    return accuracy_score(y_true, y_pred) \n",
    " \n",
    "batch_size = 500 \n",
    "num_epochs = 20\n",
    "\n",
    "# uncomment to train original baseline network\n",
    "\"\"\"\n",
    "data_loader_train = DataLoader(dataset, batch_size=batch_size, sampler=dataset.train_sampler)\n",
    "data_loader_test = DataLoader(dataset, batch_size=batch_size, sampler=dataset.test_sampler)\n",
    " \n",
    "if gpu is not None: \n",
    "    model = model.cuda() \n",
    " \n",
    "# loss criterion and optimizer \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "if gpu is not None: \n",
    "    criterion = criterion.cuda() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9,0.999)) \n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=1)\n",
    " \n",
    "running_loss = [] \n",
    "running_test_acc = [] \n",
    "start = 0 \n",
    "end = 0 \n",
    " \n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"): \n",
    "        start = time.time() \n",
    "        loss_epoch = train(model, data_loader_train, optimizer, criterion) \n",
    "        test_acc = test(model, data_loader_test) \n",
    "        print(\"Epoch %d: Training loss = %f, test accuracy = %f\" % (epoch, np.mean(loss_epoch), test_acc))\n",
    "        running_loss.append(loss_epoch) \n",
    "        running_test_acc.append(test_acc) \n",
    "        lr_scheduler.step() \n",
    "        end = time.time() \n",
    "        print(\"Epoch training time: \", end - start) \n",
    " \n",
    "        torch.save(model.state_dict(), \"FAU_CA_AI_baseline.pth\") \n",
    "\"\"\"\n",
    "\n",
    "# Load original baseline trained parameters \n",
    "savefile = \"FAU_CA_AI_baseline.pth\" \n",
    "saved_state = torch.load(savefile, map_location=torch.device(\"cpu\")) \n",
    "model.load_state_dict(saved_state) \n",
    "if gpu is not None: \n",
    "    model = model.cuda() \n",
    "\n",
    "# uncomment to get test accuracy of original baseline network\n",
    "\"\"\"\n",
    "# Set up a fresh test data loader \n",
    "dataset = radioml_18_dataset(dataset_path) \n",
    "data_loader_test = DataLoader(dataset, batch_size=batch_size, sampler=dataset.test_sampler)\n",
    " \n",
    "# Run inference on validation data \n",
    "y_exp = np.empty((0)) \n",
    "y_snr = np.empty((0)) \n",
    "y_pred = np.empty((0,len(dataset.mod_classes))) \n",
    "model.eval() \n",
    "with torch.no_grad(): \n",
    "    for data in tqdm(data_loader_test, desc=\"Batches\"): \n",
    "        inputs, target, snr = data \n",
    "        if gpu is not None: \n",
    "            inputs = inputs.cuda() \n",
    "        output = model(inputs) \n",
    "        y_pred = np.concatenate((y_pred,output.cpu())) \n",
    "        y_exp = np.concatenate((y_exp,target)) \n",
    "        y_snr = np.concatenate((y_snr,snr)) \n",
    " \n",
    "conf = np.zeros([len(dataset.mod_classes),len(dataset.mod_classes)]) \n",
    "confnorm = np.zeros([len(dataset.mod_classes),len(dataset.mod_classes)]) \n",
    "for i in range(len(y_exp)): \n",
    "    j = int(y_exp[i]) \n",
    "    k = int(np.argmax(y_pred[i,:])) \n",
    "    conf[j,k] = conf[j,k] + 1 \n",
    "    for i in range(0,len(dataset.mod_classes)): \n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:]) \n",
    "        cor = np.sum(np.diag(conf)) \n",
    "ncor = np.sum(conf) - cor \n",
    "print(\"Overall Accuracy across all SNRs of the original CNN: %f\"%(cor / (cor+ncor)))\n",
    "\n",
    "acc = [] \n",
    "for snr in dataset.snr_classes: \n",
    "    # extract classes @ SNR \n",
    "    indices_snr = (y_snr == snr).nonzero() \n",
    "    y_exp_i = y_exp[indices_snr] \n",
    "    y_pred_i = y_pred[indices_snr] \n",
    "    conf = np.zeros([len(dataset.mod_classes),len(dataset.mod_classes)]) \n",
    "    confnorm = np.zeros([len(dataset.mod_classes),len(dataset.mod_classes)])\n",
    "    for i in range(len(y_exp_i)): \n",
    "        j = int(y_exp_i[i]) \n",
    "        k = int(np.argmax(y_pred_i[i,:])) \n",
    "        conf[j,k] = conf[j,k] + 1 \n",
    "    for i in range(0,len(dataset.mod_classes)): \n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "     \n",
    "    cor = np.sum(np.diag(conf)) \n",
    "    ncor = np.sum(conf) - cor \n",
    "    acc.append(cor/(cor+ncor)) \n",
    "index = np.argmax(acc) \n",
    "print(\"Highest accuracy for snr \", dataset.snr_classes[index], acc[index]) \n",
    "print(\"Accuracy @ highest SNR (+30 dB): %f\"%(acc[-1])) \n",
    "print(\"Accuracy overall: %f\"%(np.mean(acc))) \n",
    "\"\"\"\n",
    "\n",
    "#start compression algorithm \n",
    "\n",
    "def give_p(d):\n",
    "\n",
    "        #sum of all eigen values\n",
    "        sum = np.sum(d)\n",
    "        sum_999 = 0.9998 * sum\n",
    "        temp = 0\n",
    "        p = 0\n",
    "        while temp < sum_999:\n",
    "            temp += d[p]\n",
    "            p += 1\n",
    "        return p\n",
    "\n",
    "def explained_variance(d):\n",
    "     # Get variance explained by singular values\n",
    "    explained_variance_ = (d ** 2) / (len(d) - 1)\n",
    "    total_var = explained_variance_.sum()\n",
    "    explained_variance_ratio_ = explained_variance_ / total_var\n",
    "\n",
    "    return explained_variance_ratio_\n",
    "\n",
    "N = 200\n",
    "s_list = []\n",
    "rank =64\n",
    "num_modes=2\n",
    "print(\"staring to compute mpca...\") \n",
    "print(*list(model.children()))\n",
    "\n",
    "def compute_modek_total_scatter(mode, factors):\n",
    "        scatter = 0\n",
    "\n",
    "        for m in range(X.shape[0]):\n",
    "            proj_but_k = tl.unfold(tl.tenalg.multi_mode_dot(X[m], factors, transpose=True, skip=mode), mode)\n",
    "            #print(proj_but_k.shape)\n",
    "            scatter += tl.dot(proj_but_k, proj_but_k.T)\n",
    "\n",
    "        return scatter\n",
    "\n",
    "\n",
    "data_loader_train = DataLoader(dataset, batch_size=batch_size*4, sampler=dataset.train_sampler)\n",
    "data_train = next(iter(data_loader_train))[0] \n",
    "print(\"shape train data\", len(data_train), len(data_train[0]))\n",
    "\n",
    "plot_matrix = [[0 for x in range(65)] for y in range(7)]\n",
    "for layer in range(1, 20, 3):  \n",
    "    out_model = nn.Sequential(*list(model.children())[:layer]) \n",
    "    act_final = [] \n",
    "    inputs = data_train[:N] \n",
    "    if gpu is not None: \n",
    "        inputs = inputs.cuda() \n",
    "    act_output = out_model(inputs) \n",
    "    act_output = act_output.permute(0, 2, 1) \n",
    "    print(\"printing output shape of N samples of \", layer, \"layer: \", act_output.shape)\n",
    "    num_batches = ceil(100*rank / (act_output.shape[1]*N)) \n",
    "    print(\"num batches for layer\", layer, \"is : \", num_batches) \n",
    "    inputs = data_train[:(num_batches*N)] \n",
    "    \n",
    "    if gpu is not None: \n",
    "        inputs = inputs.cuda() \n",
    "\n",
    "    act_output = out_model(inputs) \n",
    "    act_output = act_output.permute(0, 2, 1) \n",
    "    print(\"shape of act space: \", act_output.shape, type(act_output)) \n",
    "    act_output = act_output.cpu().detach().numpy() \n",
    "    print(\"shape of act output in numpy:\", act_output.shape) \n",
    "\n",
    "    mean_tensor = tl.mean(act_output, axis=0) #the mean of the input training samples TX\n",
    "    act_output = act_output - mean_tensor\n",
    "    print(\"act output shape after centering\", act_output.shape)\n",
    "    X = act_output\n",
    "    factors = [tl.ones((dim, rank)) for i, dim in enumerate(list(act_output.shape)[1:])]\n",
    "    for k in range(num_modes):\n",
    "        scatter = compute_modek_total_scatter(k, factors)\n",
    "        print(\"shape scatter\", scatter.shape)\n",
    "        U, S, _ = tl.partial_svd(scatter, n_eigenvecs = rank)\n",
    "        factors[k] = U\n",
    "        print(\"eigenvalues shape for layer\", layer, S.shape)\n",
    "        el = give_p(S)\n",
    "        print(el)\n",
    "        if (k==1):\n",
    "            s_list.append(el)\n",
    "\n",
    "        var = explained_variance(S)\n",
    "        print(var)\n",
    "        y_layer = np.cumsum(var)\n",
    "        print(y_layer)\n",
    "        print(y_layer.shape, type(y_layer), y_layer[0].shape)\n",
    "        #y_layer = np.insert(y_layer, 0, 0, axis = 0)\n",
    "        #plot_matrix[layer] = y_layer\n",
    "    \n",
    "print(s_list) \n",
    "\n",
    "filters = [s_list[0]]\n",
    "for i in range(1, len(s_list)):\n",
    "    last_elem = filters[-1]\n",
    "    if (s_list[i] > last_elem):\n",
    "        filters.append(s_list[i])\n",
    "\n",
    "print(filters)\n",
    "\n",
    "#uncomment below for saved filters\n",
    "#filters = [11, 23, 32, 36, 39, 43] \n",
    "\n",
    "#Building the network with a variable number of convolutional layers every time\n",
    "num_layers = len(filters)\n",
    "\n",
    "k= [508,250,121,57, 25, 9, 1]\n",
    "mult = k[num_layers-1]\n",
    "class CNNModuleVar(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, filters_array = []):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        self.cnn_layers = len(filters_array)\n",
    "        for l in range(self.cnn_layers):\n",
    "            self.layers.append(nn.Conv1d(input_dim, filters_array[l], kernel_size=8))\n",
    "            input_dim = filters_array[l]\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.MaxPool1d(2, stride=2))\n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_dim*mult, 128)\n",
    "        self.selu1 = nn.SELU()\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.selu2 = nn.SELU()\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for i in range(self.cnn_layers*3):\n",
    "            out = self.layers[i](out)\n",
    "            #print(out.shape)\n",
    "        out = self.flatten(out)\n",
    "        #print(out.shape)\n",
    "        out = self.fc1(out)\n",
    "        out = self.selu1(out)\n",
    "        out = self.drop1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.selu2(out)\n",
    "        out = self.drop2(out)\n",
    "        #print(out.shape)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# model2 is optimized network\n",
    "model2 = CNNModuleVar(2, 24, filters)\n",
    "model2.apply(init_weights)\n",
    "print(model2)\n",
    "\n",
    "data_loader_train = DataLoader(dataset, batch_size=batch_size, sampler=dataset.train_sampler)\n",
    "data_loader_test = DataLoader(dataset, batch_size=batch_size, sampler=dataset.test_sampler)\n",
    "\n",
    "if gpu is not None:\n",
    "    model2 = model2.cuda()\n",
    "    \n",
    "# uncomment to train optimized network\n",
    "\"\"\"\"\n",
    "# loss criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if gpu is not None:\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=0.001, betas=(0.9,0.999))\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=5, T_mult=1)\n",
    "\n",
    "running_loss = []\n",
    "running_test_acc = []\n",
    "start = 0\n",
    "end = 0\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    start = time.time()\n",
    "    loss_epoch = train(model2, data_loader_train, optimizer, criterion)\n",
    "    test_acc = test(model2, data_loader_test)\n",
    "    print(\"Epoch %d: Training loss = %f, test accuracy = %f\" % (epoch, np.mean(loss_epoch), test_acc))\n",
    "    running_loss.append(loss_epoch)\n",
    "    running_test_acc.append(test_acc)\n",
    "    lr_scheduler.step()\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "\n",
    "torch.save(model2.state_dict(), \"FAU_CA_AI_final_model.pth\")\n",
    "\n",
    "batch_size=500\n",
    "savefile = \"FAU_CA_AI_final_model.pth\"\n",
    "saved_state = torch.load(savefile, map_location=torch.device(\"cpu\"))\n",
    "model2.load_state_dict(saved_state)\n",
    "if gpu is not None:\n",
    "    model2 = model2.cuda()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# uncomment to get test accuracy of optimized network\n",
    "\"\"\"\n",
    "dataset = radioml_18_dataset(dataset_path)\n",
    "data_loader_test = DataLoader(dataset, batch_size=batch_size, sampler=dataset.test_sampler)\n",
    "\n",
    "y_exp = np.empty((0))\n",
    "y_snr = np.empty((0))\n",
    "y_pred = np.empty((0,len(dataset.mod_classes)))\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(data_loader_test, desc=\"Batches\"):\n",
    "        inputs, target, snr = data\n",
    "        if gpu is not None:\n",
    "            inputs = inputs.cuda()\n",
    "        output = model2(inputs)\n",
    "        y_pred = np.concatenate((y_pred,output.cpu()))\n",
    "        y_exp = np.concatenate((y_exp,target))\n",
    "        y_snr = np.concatenate((y_snr,snr))\n",
    "conf = np.zeros([len(dataset.mod_classes),len(dataset.mod_classes)])\n",
    "confnorm = np.zeros([len(dataset.mod_classes),len(dataset.mod_classes)])\n",
    "for i in range(len(y_exp)):\n",
    "    j = int(y_exp[i])\n",
    "    k = int(np.argmax(y_pred[i,:]))\n",
    "    conf[j,k] = conf[j,k] + 1\n",
    "for i in range(0,len(dataset.mod_classes)):\n",
    "    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "\n",
    "cor = np.sum(np.diag(conf))\n",
    "ncor = np.sum(conf) - cor\n",
    "print(\"Overall Accuracy across all SNRs of the optimized network: %f\"%(cor / (cor+ncor)))\n",
    "\"\"\"\n",
    "\n",
    "#Computing inference cost for the original baseine and the optimized network\n",
    "\n",
    "#Original baseline network\n",
    "export_onnx_path = \"FAU_CA_AI_baseline_export.onnx\"\n",
    "final_onnx_path = \"FAU_CA_AI_baseline_final.onnx\"\n",
    "cost_dict_path = \"FAU_CA_AI_baseline_cost.json\"\n",
    "\n",
    "BrevitasONNXManager.export(model.cpu(), input_t=torch.randn(1, 2, 1024), export_path=export_onnx_path);\n",
    "inference_cost(export_onnx_path, output_json=cost_dict_path, output_onnx=final_onnx_path, preprocess=True, discount_sparsity=True)\n",
    "\n",
    "def showInNetron(model_filename):\n",
    "    localhost_url = os.getenv(\"LOCALHOST_URL\")\n",
    "    netron_port = os.getenv(\"NETRON_PORT\")\n",
    "    netron.start(model_filename, address=(\"0.0.0.0\", int(netron_port)))\n",
    "    return IFrame(src=\"http://%s:%s/\" % (localhost_url, netron_port), width=\"100%\", height=400)\n",
    "\n",
    "showInNetron(final_onnx_path)\n",
    "\n",
    "with open(cost_dict_path, 'r') as f:\n",
    "    inference_cost_dict = json.load(f)\n",
    "\n",
    "bops = int(inference_cost_dict[\"total_bops\"])\n",
    "w_bits = int(inference_cost_dict[\"total_mem_w_bits\"])\n",
    "\n",
    "bops_baseline = 807699904\n",
    "w_bits_baseline = 1244936\n",
    "\n",
    "score_original_model = 0.5*(bops/bops_baseline) + 0.5*(w_bits/w_bits_baseline)\n",
    "print(\"Normalized inference cost score for the original network: %f\" % score_original_model)\n",
    "\n",
    "# Optimized network\n",
    "\n",
    "export_onnx_path = \"FAU_CA_AI_final_model_export.onnx\" \n",
    "final_onnx_path = \"FAU_CA_AI_final_model_final.onnx\" \n",
    "cost_dict_path = \"FAU_CA_AI_final_model_cost.json\"\n",
    "\n",
    "BrevitasONNXManager.export(model2.cpu(), input_t=torch.randn(1, 2, 1024), export_path=export_onnx_path)\n",
    "inference_cost(export_onnx_path, output_json=cost_dict_path, output_onnx=final_onnx_path)\n",
    " \n",
    "showInNetron(final_onnx_path) \n",
    " \n",
    "with open(cost_dict_path, 'r') as f: \n",
    "    inference_cost_dict = json.load(f)\n",
    "\n",
    "bops = int(inference_cost_dict[\"total_bops\"]) \n",
    "w_bits = int(inference_cost_dict[\"total_mem_w_bits\"]) \n",
    " \n",
    "bops_baseline = 807699904 \n",
    "w_bits_baseline = 1244936\n",
    "\n",
    "score_optimized_model = 0.5*(bops/bops_baseline) + 0.5*(w_bits/w_bits_baseline) \n",
    "print(\"Normalized inference cost score for the optimized network: %f\" % score_optimized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255bfb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
